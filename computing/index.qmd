---
title: "Lab Computing"
subtitle: "Setup guides, coding standards, and cluster workflows"
---

This page is your hub for all computing-related resources in the lab. Whether you're setting up your environment, learning our coding standards, or running jobs on Longleaf, you'll find links to the relevant guides here.

## Getting Started

New to the lab or setting up a new machine? Start here:

| Guide | Description |
|-------|-------------|
| [Your First Project](../onboarding/first-project.qmd) | Starting your first research project |
| [Tools Setup](../onboarding/tools-setup.qmd) | Overview of required tools and configuration |
| [Longleaf Setup](../onboarding/setup-longleaf.qmd) | UNC HPC cluster access and configuration |
| [Local Development Setup](../onboarding/setup-local.qmd) | R, RStudio, VS Code on your machine |
| [Git & GitHub Setup](../onboarding/setup-git.qmd) | Version control configuration |

## Coding Standards

Our lab follows specific conventions for reproducibility and collaboration:

| Guide | Description |
|-------|-------------|
| [Coding Standards Overview](../coding-standards/index.qmd) | Summary of all coding conventions |
| [R Style Guide](../coding-standards/r-style.qmd) | Base R + data.table conventions |
| [Python Style Guide](../coding-standards/python-style.qmd) | Python conventions and formatting |
| [Git Practices](../coding-standards/git-practices.qmd) | Commits, branches, PRs, code review |
| [Targets Pipeline Guide](../coding-standards/targets-pipeline.qmd) | Reproducible workflows with {targets} |

## Project Consistency

Keep your code and documentation in sync:

| Guide | Description |
|-------|-------------|
| [Project Consistency Framework](../project-consistency/index.qmd) | Overview of the consistency system |
| [Centralized Configuration](../project-consistency/config-values.qmd) | Single source of truth for values |
| [Data Provenance](../project-consistency/data-provenance.qmd) | Tracking data origins and transformations |

## HPC Resources

::: {.callout-tip}
## New to Longleaf?
Start with [Longleaf Setup](../onboarding/setup-longleaf.qmd) for initial access, OnDemand, and basic configuration. The sections below cover **advanced usage** for production workflows.
:::

## Slurm Job Scheduler

### Basic Commands

```bash
# Submit a job
sbatch my_script.sh

# Check queue
squeue -u $USER

# Cancel a job
scancel JOB_ID

# Job information
scontrol show job JOB_ID
sacct -j JOB_ID
```

### Example Batch Script

```bash
#!/bin/bash
#SBATCH --job-name=my_analysis
#SBATCH --partition=general
#SBATCH --time=24:00:00
#SBATCH --mem=16G
#SBATCH --cpus-per-task=4
#SBATCH --output=logs/slurm_%j.out
#SBATCH --error=logs/slurm_%j.err

module load r/4.3.0

Rscript my_analysis.R
```

### Partitions

| Partition | Time Limit | Use Case |
|-----------|------------|----------|
| `debug` | 4 hours | Testing, quick jobs |
| `general` | 7 days | Standard jobs |
| `bigmem` | 7 days | High memory jobs |
| `gpu` | 7 days | GPU computing |

### Resource Guidelines

| Job Type | CPUs | Memory | Time |
|----------|------|--------|------|
| Light analysis | 1-2 | 4-8 GB | 1-4 hours |
| Simulation (single) | 4 | 8-16 GB | 4-24 hours |
| Heavy simulation | 8-16 | 32-64 GB | 24-48 hours |
| Parallel (targets) | 4/worker | 4 GB/CPU | varies |

## Targets + Slurm Integration

The `{targets}` package with `{crew.cluster}` enables distributed pipelines on Longleaf.

### Configuration

```r
# In _targets.R
library(crew)
library(crew.cluster)

tar_option_set(
  controller = crew_controller_slurm(
    name = "longleaf",
    workers = 20,
    slurm_partition = "general",
    slurm_time_minutes = 1440,
    slurm_cpus_per_task = 4,
    slurm_memory_gigabytes_per_cpu = 4,
    slurm_log_output = "logs/slurm_%j.out"
  )
)
```

### Running Pipeline on Cluster

```bash
# Submit controller job
sbatch run_pipeline.sh
```

`run_pipeline.sh`:
```bash
#!/bin/bash
#SBATCH --job-name=targets_controller
#SBATCH --time=48:00:00
#SBATCH --mem=8G
#SBATCH --cpus-per-task=2
#SBATCH --output=logs/controller_%j.out

module load r/4.3.0
Rscript -e "targets::tar_make()"
```

### Monitoring

```bash
# Watch queue
watch -n 10 squeue -u $USER

# Check targets progress
Rscript -e "targets::tar_progress()"

# View logs
tail -f logs/slurm_*.out
```

## Array Jobs

For running many similar jobs (e.g., simulation replicates):

```bash
#!/bin/bash
#SBATCH --job-name=sim_array
#SBATCH --array=1-100
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH --time=2:00:00
#SBATCH --output=logs/sim_%A_%a.out

module load r/4.3.0

Rscript run_simulation.R $SLURM_ARRAY_TASK_ID
```

In R:
```r
# run_simulation.R
args <- commandArgs(trailingOnly = TRUE)
task_id <- as.integer(args[1])

set.seed(task_id)
# ... run simulation with this seed
```

## Shared R Package Library

### Using Lab Packages

Add to your `~/.Rprofile` on Longleaf:

```r
.libPaths(c(
  "~/R/library",
  "/proj/rashidlab/R-packages",
  .libPaths()
))
```

### Installing to Shared Library

For packages everyone needs:

```r
# Install to shared lab directory (ask first!)
install.packages("mypackage", lib = "/proj/rashidlab/R-packages")
```

## Quick Mode for Testing

Use reduced parameters for local testing before cluster submission:

```bash
# Environment variable approach
QUICK_MODE=1 Rscript my_analysis.R
```

```r
# In R script
if (Sys.getenv("QUICK_MODE") == "1") {
  n_reps <- 100
} else {
  n_reps <- 10000
}
```

## Claude Code on Longleaf

Claude Code can help manage HPC workflows directly from the command line.

### Setup

```bash
# Install on Longleaf
curl -fsSL https://claude.ai/install.sh | sh
claude login
```

### Common Tasks

```
> Submit this job to Slurm with 4 CPUs and 16GB RAM
> Check my queue status
> Configure targets to use 20 Slurm workers
> Debug why my job failed
```

### Long Sessions

Use tmux to keep Claude running:

```bash
tmux new -s claude
claude
# Ctrl-b, d to detach
# tmux attach -t claude to reattach
```

See the [Claude Code HPC Guide](../claude-code/hpc-usage.qmd) for comprehensive documentation.

## Related Resources

| Resource | Description |
|----------|-------------|
| [Claude Code Guide](../claude-code/index.qmd) | AI-assisted development and coding |
| [Lab Branding](../branding/index.qmd) | Presentations, colors, logos, and visual identity |
| [Lab Policies](../policies/index.qmd) | Communication, data management, publications |
| [Project Templates](../templates/index.qmd) | Starters for research projects and papers |

## Getting Help

- **Longleaf issues**: research@unc.edu
- **Lab computing questions**: `#computing` Teams channel
- **Targets/pipeline help**: [targets documentation](https://books.ropensci.org/targets/)
- **Claude Code help**: [Claude Code Guide](../claude-code/index.qmd)
- **Initial setup**: [Tools Setup Guide](../onboarding/tools-setup.qmd)
