---
title: "Computing Resources"
subtitle: "Advanced Slurm usage, targets pipelines, and cluster workflows"
---

::: {.callout-tip}
## New to Longleaf?
Start with [Tools Setup](../onboarding/tools-setup.qmd) for initial access, OnDemand, and basic configuration. This page covers **advanced usage** for production workflows.
:::

## Slurm Job Scheduler

### Basic Commands

```bash
# Submit a job
sbatch my_script.sh

# Check queue
squeue -u $USER

# Cancel a job
scancel JOB_ID

# Job information
scontrol show job JOB_ID
sacct -j JOB_ID
```

### Example Batch Script

```bash
#!/bin/bash
#SBATCH --job-name=my_analysis
#SBATCH --partition=general
#SBATCH --time=24:00:00
#SBATCH --mem=16G
#SBATCH --cpus-per-task=4
#SBATCH --output=logs/slurm_%j.out
#SBATCH --error=logs/slurm_%j.err

module load r/4.3.0

Rscript my_analysis.R
```

### Partitions

| Partition | Time Limit | Use Case |
|-----------|------------|----------|
| `debug` | 4 hours | Testing, quick jobs |
| `general` | 7 days | Standard jobs |
| `bigmem` | 7 days | High memory jobs |
| `gpu` | 7 days | GPU computing |

### Resource Guidelines

| Job Type | CPUs | Memory | Time |
|----------|------|--------|------|
| Light analysis | 1-2 | 4-8 GB | 1-4 hours |
| Simulation (single) | 4 | 8-16 GB | 4-24 hours |
| Heavy simulation | 8-16 | 32-64 GB | 24-48 hours |
| Parallel (targets) | 4/worker | 4 GB/CPU | varies |

## Targets + Slurm Integration

The `{targets}` package with `{crew.cluster}` enables distributed pipelines on Longleaf.

### Configuration

```r
# In _targets.R
library(crew)
library(crew.cluster)

tar_option_set(
  controller = crew_controller_slurm(
    name = "longleaf",
    workers = 20,
    slurm_partition = "general",
    slurm_time_minutes = 1440,
    slurm_cpus_per_task = 4,
    slurm_memory_gigabytes_per_cpu = 4,
    slurm_log_output = "logs/slurm_%j.out"
  )
)
```

### Running Pipeline on Cluster

```bash
# Submit controller job
sbatch run_pipeline.sh
```

`run_pipeline.sh`:
```bash
#!/bin/bash
#SBATCH --job-name=targets_controller
#SBATCH --time=48:00:00
#SBATCH --mem=8G
#SBATCH --cpus-per-task=2
#SBATCH --output=logs/controller_%j.out

module load r/4.3.0
Rscript -e "targets::tar_make()"
```

### Monitoring

```bash
# Watch queue
watch -n 10 squeue -u $USER

# Check targets progress
Rscript -e "targets::tar_progress()"

# View logs
tail -f logs/slurm_*.out
```

## Array Jobs

For running many similar jobs (e.g., simulation replicates):

```bash
#!/bin/bash
#SBATCH --job-name=sim_array
#SBATCH --array=1-100
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH --time=2:00:00
#SBATCH --output=logs/sim_%A_%a.out

module load r/4.3.0

Rscript run_simulation.R $SLURM_ARRAY_TASK_ID
```

In R:
```r
# run_simulation.R
args <- commandArgs(trailingOnly = TRUE)
task_id <- as.integer(args[1])

set.seed(task_id)
# ... run simulation with this seed
```

## Shared R Package Library

### Using Lab Packages

Add to your `~/.Rprofile` on Longleaf:

```r
.libPaths(c(
  "~/R/library",
  "/proj/rashidlab/R-packages",
  .libPaths()
))
```

### Installing to Shared Library

For packages everyone needs:

```r
# Install to shared lab directory (ask first!)
install.packages("mypackage", lib = "/proj/rashidlab/R-packages")
```

## Quick Mode for Testing

Use reduced parameters for local testing before cluster submission:

```bash
# Environment variable approach
QUICK_MODE=1 Rscript my_analysis.R
```

```r
# In R script
if (Sys.getenv("QUICK_MODE") == "1") {
  n_reps <- 100
} else {
  n_reps <- 10000
}
```

## Getting Help

- **Longleaf issues**: research@unc.edu
- **Lab computing questions**: `#computing` Teams channel
- **Targets/pipeline help**: [targets documentation](https://books.ropensci.org/targets/)
- **Initial setup**: [Tools Setup Guide](../onboarding/tools-setup.qmd)
